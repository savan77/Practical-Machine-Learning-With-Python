{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Machine Learning With Python - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:verdana; font-size:15px\">In the <a href=\"https://savan77.github.io/blog/machine-learning-part1.html\"> previous post</a>, I explained what is machine learning, types of machine learning, linear regression, logistic regression, various issues that we need to consider such as overfitting and at last I explained what really learning is in machine learning. In <a href=\"https://savan77.github.io/blog/lab-machine-learning-part1.html\">lab session</a>, I explained how to implement algorithms and concepts that I explained in theory session using Python. Check out <a href=\"https://github.com/savan77/Practical-Machine-Learning-With-Python/\">Github repository</a> of this series.</p>\n",
    "<p style=\"font-family:verdana; font-size:15px;\">\n",
    "In this session, I will explain some easy yet powerful machine learning algorithms such as <b> naive bayes, support vector machine and decision trees</b>. From now onwards, I will not make seperate part for theory and lab session. Instead, I will integrate theory with code in jupyter notebook. If you are unfamiliar with Jupyter notebooks, please go through <a href=\"http://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Notebook%20Basics.html\"> Jupyter Notebook Basics Guide </a>.</p>\n",
    "<h3 style=\"font-family:verdana; margin-left:30px\">Index</a></h3>\n",
    "<ul>\n",
    "<a href=\"#naive\"><li> <p style=\"font-family:verdana; font-size:15px\">\n",
    "Naive Bayes </p></li></a>\n",
    "<a href=\"#svm\"><li><p style=\"font-family:verdana; font-size:15px\">\n",
    " Support Vector Machines</p></li></a>\n",
    " <a href=\"#dt\"><li><p style=\"font-family:verdana; font-size:15px\">\n",
    " Decision Tree</p></li></a>\n",
    " <a href=\"#ensemble\"><li><p style=\"font-family:verdana; font-size:15px\">\n",
    " Ensemble Methods</p></li></a>\n",
    " <a href=\"#ex\"><li><p style=\"font-family:verdana; font-size:15px\">\n",
    " Exercise</p></li></a>\n",
    "</ul></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p style=\"font-family:verdana; font-size:15px\" id=\"naive\"><b> Naive Bayes </b> is a supervised learning algorithm which is based on <a href=\"https://en.wikipedia.org/wiki/Bayes%27_theorem\"><b> bayes theorem </b></a>. Naive Bayes is a widely used classification algorithm. Here, word <b> naive</b> comes from the assumption of independence among features. That is, if we have a feature vector (input vector) (x<sub>1</sub>, x<sub>2</sub>,...,x<sub>n</sub>), x<sub>i</sub><sup>'</sup>s are conditionally independent given <i>y</i>. We can write bayes theorem as follows :<br><br>\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "P( y | x ) = \\frac{P(y)P(x | y)}{P(x)}\n",
    "\\end{align}\n",
    "\n",
    "where,<br><br>\n",
    "P(x) is the prior probability of a feature.<br>\n",
    "P(x | y) is the probability of a feature given target. It's also known as likelihood.<br>\n",
    "P(y) is the prior probability of a target or class in case of classification.<br>\n",
    "p(y | x) is the posterior probability of target given feature.<br>\n",
    "<br>\n",
    "when we have more than one feature then we can rewrite this equation as :<br><br>\n",
    "\n",
    "\\begin{align}\n",
    "P( y | x_1,...,x_n) = \\frac{P(y)P(x_1,...,x_n | y)}{P(x_1,...,x_n)}\n",
    "\\end{align}\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:verdana; font-size:15px\">Consider an example for spam classification, our input or feature vector will be a set of words and output will be spam or ham (1 or 0). In naive bayes, we calculate probability of each class(spam or ham) given feature vector and class with maximum probability becomes our output. Our task is to solve above equation for each class. Now, let us dig deeper into this equation and see how we can use this equation to find the probability of each class.<br>\n",
    "Using the naive bayes assumption we can write :<br><br>\n",
    "\\begin{align}\n",
    "P(x_i | y, x_1,..,x_{i-1},x{i+1},..,x_n) = P(x_i | y)\n",
    "\\end{align}\n",
    "\n",
    "We can rewrite bayes theorem as follows : \n",
    "<br><br>\n",
    "\\begin{align}\n",
    "P( y | x_1,...,x_n) = \\frac{P(y)\\prod_{i=1}^{n}P(x_i| y)}{P(x_1,...,x_n)}\n",
    "\\end{align}<br><br>\n",
    "\n",
    "but we know that <b> P(x<sub>1</sub>, x<sub>2</sub>, .., x<sub>n</sub>)</b> is constant given the input. So we can say that\n",
    "<br><br>\n",
    "\n",
    "\\begin{align}\n",
    "P(y|x_1,...,x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i|y)\\end{align}\n",
    "<br>\n",
    "For classification rule (where we want to find the class with maximum probability), we can write equation as :\n",
    "<br><br>\n",
    "\\begin{align}\n",
    "\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i|y)\n",
    "\\end{align}\n",
    "<br>\n",
    "Now, we can use <a href=\"https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation\"> Maximum a Posteriori</a> estimation to estimate both P(y) and P(x<sub>i</sub>|y). Here, P(y) = samples with class y / total number of sample, in other words, frequency of class y in training data. <br><br>\n",
    "\n",
    "We can make several variants of naive bayes by using different distribution for P(x<sub>i</sub>|y). Widely used Naive Bayes variants are <a href=\"https://en.wikipedia.org/wiki/Normal_distribution\"> Gaussian Naive Bayes </a>, <a href=\"https://en.wikipedia.org/wiki/Multinomial_distribution\"> Multinomial Naive Bayes </a> and <a href=\"https://en.wikipedia.org/wiki/Bernoulli_distribution\"> Bernoulli Naive Bayes </a>.\n",
    "\n",
    "Now, we will implement Naive Bayes algorithm in scikit-learn.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n",
      "[1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\savan77\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#we will use iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "#load the dataset\n",
    "data = load_iris()\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(data.data, data.target)\n",
    "\n",
    "#evalaute\n",
    "print(model.score(data.data, data.target))\n",
    "\n",
    "#predict\n",
    "print(model.predict([4.2, 3, 0.9, 2.1])) #0 = setosa,1 = versicolor, and 2 = virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p style=\"font-family:verdana; font-size:15px\" id=\"svm\"><b> Support Vector Machines</b> are supervised learning models which can be used for both classification and regression. SVMs are among the best supervised learning algorithms. It is effective in high dimensional space and it is memory efficient as well.\n",
    "\n",
    "Consider a binary classification problem, where the task is to assign a one of the two labels to given input. We plot each data item as a point in n-dimensional space as follows:</p>\n",
    "\n",
    "![title](../images/svm1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:verdana; font-size:15px\">We can perform classification by finding the hyperplane that differentiate the two classes very well. As you can see in the above image, we can draw m number of hyperplanes. How do we find the best one? We can find the optimal hyperplane by maximizing the <b> margin </b>.</p>\n",
    "![title](../images/svm2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:verdana; font-size:15px\">We define margin as a twice of the distance between the hyperplane and the nearest sample points to the hyperplane. This points are known as <b>support vector</b>. They known as support vectors because they hold up optimal hyperplane. In above figure, support vectors are represented with filled color.\n",
    "\n",
    "Consider a first hyperplane in figure-1 which touches the two sample points(red). Although it classifies all the examples correctly, but the problem is our hyperplane is close to the so many sample points and other red examples might fall on the other side of the hyperplane. This problem can be solved by choosing a hyperplane which is farthest away from the sample points. It turns out that this type of model generalize very well. This optimal hyperplane is also known as <b> maximum margin separator</b>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p style=\"font-family:verdana; font-size:15px\">We know that we want hyperplane with maximum margin and we also discussed why we want this. Now, let us learn how to find this optimal hyperplane? Before that, please note in case of SVMs, we represent class labels with +1 and -1 instead of 0 and 1(Binary Valued Labels). Here, we represent each hyperplane, the optimal one, negative and positive hyperplane(dashed lines) with linear equations - w<sup>T</sup>x + b = 0, w<sup>T</sup>x+b = -1 and w<sup>T</sup>x + b = +1 respectively. The left most dashed line is negative hyperplane. We represent red points with x<sub>-</sub> and blue points with x<sub>+</sub>. To derive the equation for a margin let us substract equations of negative and positive hyperplane from each other.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:verdana; font-size:15px\">\\begin{align}\n",
    "w^T(x_+ - x_-) = 2\n",
    "\\end{align}<br><br>\n",
    "\n",
    "Adding length of the vector w to normalize this,<br>\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{w^T(x_+ - x_-)}{||w||} = \\frac{2}{||w||} \n",
    "\\end{align}\n",
    "<br><br>\n",
    "where, <b> 2/||w||</b> is the margin.\n",
    "\n",
    "Now the objective of the SVM becomes maximization of the margin under the constraint that samples are classified correctly.\n",
    "\\begin{align}\n",
    "w^T x^{(i)} + b >= 1  \\hspace{1cm} if \\hspace{1cm} y^{(i)} = +1 \\newline\n",
    "w^T x^{(i)} + b < -1  \\hspace{1cm} if \\hspace{1cm} y^{(i)} = -1\n",
    "\\end{align}\n",
    "<br><br>\n",
    "This can also be written more compractly as \n",
    "\\begin{align}\n",
    "y^{(i)} ( w_0 + w^T x^{(i)}) >= 1 \\forall_i\n",
    "\\end{align}</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:verdana; font-size:15px\">In practice, it is easier to minimize the below given reciprocal term \\begin{align} \\frac{1}{2} ||w||^2 \\end{align}. This is the quadratic programming problem with the linear constraint.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:verdana; font-size:15px\">In the case of inherently noisy data, we may not want a linear hyperplane in high-dimensional space. Rather, we'd like a decision surface in low dimensional space that does not clearly seperate the classes, but reflects the reality of the noisy data. That is possible with the <b> soft margin classifier</b>, which allows examples to fall on the wrong side of the decision boundary, but assigns them a penalty proportional to the distance required to move them back on the correct side. In soft margin classifier, we add slack variables to the linear constraint.<br><br>\n",
    "\\begin{align}\n",
    "y_{(i)} (w^T x_i + b ) >= 1 - \\xi \\hspace{1cm} for \\hspace{1cm} i = 1,..,N\n",
    "\\end{align}\n",
    "\n",
    "Now, our objective to minimize is<br><br>\n",
    "\\begin{align}\n",
    "\\frac{1}{2} ||w||^2 + C (\\sum_{i} \\xi^{(i)})\n",
    "\\end{align}\n",
    "\n",
    "C is the <b> regularization </b> parameter. Small C allows constraint to be easily ignored and results in large margin whereas large C makes constraints hard to ignore and results in narrow margin. This is still a quadratic optimization problem and there is a unique minimum. \n",
    "\n",
    "Now let us implement linear SVM classifier in Python using sklearn. We will use iris dataset</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import the dependencies\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "#load dataset\n",
    "dataset = load_iris()\n",
    "data = dataset.data\n",
    "target = dataset.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In machine learning, we always need to do some preprocessing to make our dataset suitable for the learning algorithm. I will introduce few preprocessing techniques as we go through various algorithms. Here, we will perform feature scaling which is required for optimal performance. Feature scaling is used to standardize the range of features of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -9.00681170e-01,   1.03205722e+00,  -1.34127240e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -1.14301691e+00,  -1.24957601e-01,  -1.34127240e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -1.38535265e+00,   3.37848329e-01,  -1.39813811e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -1.50652052e+00,   1.06445364e-01,  -1.28440670e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -1.02184904e+00,   1.26346019e+00,  -1.34127240e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -5.37177559e-01,   1.95766909e+00,  -1.17067529e+00,\n",
       "         -1.05003079e+00],\n",
       "       [ -1.50652052e+00,   8.00654259e-01,  -1.34127240e+00,\n",
       "         -1.18150376e+00],\n",
       "       [ -1.02184904e+00,   8.00654259e-01,  -1.28440670e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -1.74885626e+00,  -3.56360566e-01,  -1.34127240e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -1.14301691e+00,   1.06445364e-01,  -1.28440670e+00,\n",
       "         -1.44444970e+00],\n",
       "       [ -5.37177559e-01,   1.49486315e+00,  -1.28440670e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -1.26418478e+00,   8.00654259e-01,  -1.22754100e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -1.26418478e+00,  -1.24957601e-01,  -1.34127240e+00,\n",
       "         -1.44444970e+00],\n",
       "       [ -1.87002413e+00,  -1.24957601e-01,  -1.51186952e+00,\n",
       "         -1.44444970e+00],\n",
       "       [ -5.25060772e-02,   2.18907205e+00,  -1.45500381e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -1.73673948e-01,   3.11468391e+00,  -1.28440670e+00,\n",
       "         -1.05003079e+00],\n",
       "       [ -5.37177559e-01,   1.95766909e+00,  -1.39813811e+00,\n",
       "         -1.05003079e+00],\n",
       "       [ -9.00681170e-01,   1.03205722e+00,  -1.34127240e+00,\n",
       "         -1.18150376e+00],\n",
       "       [ -1.73673948e-01,   1.72626612e+00,  -1.17067529e+00,\n",
       "         -1.18150376e+00],\n",
       "       [ -9.00681170e-01,   1.72626612e+00,  -1.28440670e+00,\n",
       "         -1.18150376e+00],\n",
       "       [ -5.37177559e-01,   8.00654259e-01,  -1.17067529e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -9.00681170e-01,   1.49486315e+00,  -1.28440670e+00,\n",
       "         -1.05003079e+00],\n",
       "       [ -1.50652052e+00,   1.26346019e+00,  -1.56873522e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -9.00681170e-01,   5.69251294e-01,  -1.17067529e+00,\n",
       "         -9.18557817e-01],\n",
       "       [ -1.26418478e+00,   8.00654259e-01,  -1.05694388e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -1.02184904e+00,  -1.24957601e-01,  -1.22754100e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -1.02184904e+00,   8.00654259e-01,  -1.22754100e+00,\n",
       "         -1.05003079e+00],\n",
       "       [ -7.79513300e-01,   1.03205722e+00,  -1.28440670e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -7.79513300e-01,   8.00654259e-01,  -1.34127240e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -1.38535265e+00,   3.37848329e-01,  -1.22754100e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -1.26418478e+00,   1.06445364e-01,  -1.22754100e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -5.37177559e-01,   8.00654259e-01,  -1.28440670e+00,\n",
       "         -1.05003079e+00],\n",
       "       [ -7.79513300e-01,   2.42047502e+00,  -1.28440670e+00,\n",
       "         -1.44444970e+00],\n",
       "       [ -4.16009689e-01,   2.65187798e+00,  -1.34127240e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -1.14301691e+00,   1.06445364e-01,  -1.28440670e+00,\n",
       "         -1.44444970e+00],\n",
       "       [ -1.02184904e+00,   3.37848329e-01,  -1.45500381e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -4.16009689e-01,   1.03205722e+00,  -1.39813811e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -1.14301691e+00,   1.06445364e-01,  -1.28440670e+00,\n",
       "         -1.44444970e+00],\n",
       "       [ -1.74885626e+00,  -1.24957601e-01,  -1.39813811e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -9.00681170e-01,   8.00654259e-01,  -1.28440670e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -1.02184904e+00,   1.03205722e+00,  -1.39813811e+00,\n",
       "         -1.18150376e+00],\n",
       "       [ -1.62768839e+00,  -1.74477836e+00,  -1.39813811e+00,\n",
       "         -1.18150376e+00],\n",
       "       [ -1.74885626e+00,   3.37848329e-01,  -1.39813811e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -1.02184904e+00,   1.03205722e+00,  -1.22754100e+00,\n",
       "         -7.87084847e-01],\n",
       "       [ -9.00681170e-01,   1.72626612e+00,  -1.05694388e+00,\n",
       "         -1.05003079e+00],\n",
       "       [ -1.26418478e+00,  -1.24957601e-01,  -1.34127240e+00,\n",
       "         -1.18150376e+00],\n",
       "       [ -9.00681170e-01,   1.72626612e+00,  -1.22754100e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -1.50652052e+00,   3.37848329e-01,  -1.34127240e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -6.58345429e-01,   1.49486315e+00,  -1.28440670e+00,\n",
       "         -1.31297673e+00],\n",
       "       [ -1.02184904e+00,   5.69251294e-01,  -1.34127240e+00,\n",
       "         -1.31297673e+00],\n",
       "       [  1.40150837e+00,   3.37848329e-01,   5.35295827e-01,\n",
       "          2.64698913e-01],\n",
       "       [  6.74501145e-01,   3.37848329e-01,   4.21564419e-01,\n",
       "          3.96171883e-01],\n",
       "       [  1.28034050e+00,   1.06445364e-01,   6.49027235e-01,\n",
       "          3.96171883e-01],\n",
       "       [ -4.16009689e-01,  -1.74477836e+00,   1.37235899e-01,\n",
       "          1.33225943e-01],\n",
       "       [  7.95669016e-01,  -5.87763531e-01,   4.78430123e-01,\n",
       "          3.96171883e-01],\n",
       "       [ -1.73673948e-01,  -5.87763531e-01,   4.21564419e-01,\n",
       "          1.33225943e-01],\n",
       "       [  5.53333275e-01,   5.69251294e-01,   5.35295827e-01,\n",
       "          5.27644853e-01],\n",
       "       [ -1.14301691e+00,  -1.51337539e+00,  -2.60824029e-01,\n",
       "         -2.61192967e-01],\n",
       "       [  9.16836886e-01,  -3.56360566e-01,   4.78430123e-01,\n",
       "          1.33225943e-01],\n",
       "       [ -7.79513300e-01,  -8.19166497e-01,   8.03701950e-02,\n",
       "          2.64698913e-01],\n",
       "       [ -1.02184904e+00,  -2.43898725e+00,  -1.47092621e-01,\n",
       "         -2.61192967e-01],\n",
       "       [  6.86617933e-02,  -1.24957601e-01,   2.50967307e-01,\n",
       "          3.96171883e-01],\n",
       "       [  1.89829664e-01,  -1.97618132e+00,   1.37235899e-01,\n",
       "         -2.61192967e-01],\n",
       "       [  3.10997534e-01,  -3.56360566e-01,   5.35295827e-01,\n",
       "          2.64698913e-01],\n",
       "       [ -2.94841818e-01,  -3.56360566e-01,  -9.02269170e-02,\n",
       "          1.33225943e-01],\n",
       "       [  1.03800476e+00,   1.06445364e-01,   3.64698715e-01,\n",
       "          2.64698913e-01],\n",
       "       [ -2.94841818e-01,  -1.24957601e-01,   4.21564419e-01,\n",
       "          3.96171883e-01],\n",
       "       [ -5.25060772e-02,  -8.19166497e-01,   1.94101603e-01,\n",
       "         -2.61192967e-01],\n",
       "       [  4.32165405e-01,  -1.97618132e+00,   4.21564419e-01,\n",
       "          3.96171883e-01],\n",
       "       [ -2.94841818e-01,  -1.28197243e+00,   8.03701950e-02,\n",
       "         -1.29719997e-01],\n",
       "       [  6.86617933e-02,   3.37848329e-01,   5.92161531e-01,\n",
       "          7.90590793e-01],\n",
       "       [  3.10997534e-01,  -5.87763531e-01,   1.37235899e-01,\n",
       "          1.33225943e-01],\n",
       "       [  5.53333275e-01,  -1.28197243e+00,   6.49027235e-01,\n",
       "          3.96171883e-01],\n",
       "       [  3.10997534e-01,  -5.87763531e-01,   5.35295827e-01,\n",
       "          1.75297293e-03],\n",
       "       [  6.74501145e-01,  -3.56360566e-01,   3.07833011e-01,\n",
       "          1.33225943e-01],\n",
       "       [  9.16836886e-01,  -1.24957601e-01,   3.64698715e-01,\n",
       "          2.64698913e-01],\n",
       "       [  1.15917263e+00,  -5.87763531e-01,   5.92161531e-01,\n",
       "          2.64698913e-01],\n",
       "       [  1.03800476e+00,  -1.24957601e-01,   7.05892939e-01,\n",
       "          6.59117823e-01],\n",
       "       [  1.89829664e-01,  -3.56360566e-01,   4.21564419e-01,\n",
       "          3.96171883e-01],\n",
       "       [ -1.73673948e-01,  -1.05056946e+00,  -1.47092621e-01,\n",
       "         -2.61192967e-01],\n",
       "       [ -4.16009689e-01,  -1.51337539e+00,   2.35044910e-02,\n",
       "         -1.29719997e-01],\n",
       "       [ -4.16009689e-01,  -1.51337539e+00,  -3.33612130e-02,\n",
       "         -2.61192967e-01],\n",
       "       [ -5.25060772e-02,  -8.19166497e-01,   8.03701950e-02,\n",
       "          1.75297293e-03],\n",
       "       [  1.89829664e-01,  -8.19166497e-01,   7.62758643e-01,\n",
       "          5.27644853e-01],\n",
       "       [ -5.37177559e-01,  -1.24957601e-01,   4.21564419e-01,\n",
       "          3.96171883e-01],\n",
       "       [  1.89829664e-01,   8.00654259e-01,   4.21564419e-01,\n",
       "          5.27644853e-01],\n",
       "       [  1.03800476e+00,   1.06445364e-01,   5.35295827e-01,\n",
       "          3.96171883e-01],\n",
       "       [  5.53333275e-01,  -1.74477836e+00,   3.64698715e-01,\n",
       "          1.33225943e-01],\n",
       "       [ -2.94841818e-01,  -1.24957601e-01,   1.94101603e-01,\n",
       "          1.33225943e-01],\n",
       "       [ -4.16009689e-01,  -1.28197243e+00,   1.37235899e-01,\n",
       "          1.33225943e-01],\n",
       "       [ -4.16009689e-01,  -1.05056946e+00,   3.64698715e-01,\n",
       "          1.75297293e-03],\n",
       "       [  3.10997534e-01,  -1.24957601e-01,   4.78430123e-01,\n",
       "          2.64698913e-01],\n",
       "       [ -5.25060772e-02,  -1.05056946e+00,   1.37235899e-01,\n",
       "          1.75297293e-03],\n",
       "       [ -1.02184904e+00,  -1.74477836e+00,  -2.60824029e-01,\n",
       "         -2.61192967e-01],\n",
       "       [ -2.94841818e-01,  -8.19166497e-01,   2.50967307e-01,\n",
       "          1.33225943e-01],\n",
       "       [ -1.73673948e-01,  -1.24957601e-01,   2.50967307e-01,\n",
       "          1.75297293e-03],\n",
       "       [ -1.73673948e-01,  -3.56360566e-01,   2.50967307e-01,\n",
       "          1.33225943e-01],\n",
       "       [  4.32165405e-01,  -3.56360566e-01,   3.07833011e-01,\n",
       "          1.33225943e-01],\n",
       "       [ -9.00681170e-01,  -1.28197243e+00,  -4.31421141e-01,\n",
       "         -1.29719997e-01],\n",
       "       [ -1.73673948e-01,  -5.87763531e-01,   1.94101603e-01,\n",
       "          1.33225943e-01],\n",
       "       [  5.53333275e-01,   5.69251294e-01,   1.27454998e+00,\n",
       "          1.71090158e+00],\n",
       "       [ -5.25060772e-02,  -8.19166497e-01,   7.62758643e-01,\n",
       "          9.22063763e-01],\n",
       "       [  1.52267624e+00,  -1.24957601e-01,   1.21768427e+00,\n",
       "          1.18500970e+00],\n",
       "       [  5.53333275e-01,  -3.56360566e-01,   1.04708716e+00,\n",
       "          7.90590793e-01],\n",
       "       [  7.95669016e-01,  -1.24957601e-01,   1.16081857e+00,\n",
       "          1.31648267e+00],\n",
       "       [  2.12851559e+00,  -1.24957601e-01,   1.61574420e+00,\n",
       "          1.18500970e+00],\n",
       "       [ -1.14301691e+00,  -1.28197243e+00,   4.21564419e-01,\n",
       "          6.59117823e-01],\n",
       "       [  1.76501198e+00,  -3.56360566e-01,   1.44514709e+00,\n",
       "          7.90590793e-01],\n",
       "       [  1.03800476e+00,  -1.28197243e+00,   1.16081857e+00,\n",
       "          7.90590793e-01],\n",
       "       [  1.64384411e+00,   1.26346019e+00,   1.33141568e+00,\n",
       "          1.71090158e+00],\n",
       "       [  7.95669016e-01,   3.37848329e-01,   7.62758643e-01,\n",
       "          1.05353673e+00],\n",
       "       [  6.74501145e-01,  -8.19166497e-01,   8.76490051e-01,\n",
       "          9.22063763e-01],\n",
       "       [  1.15917263e+00,  -1.24957601e-01,   9.90221459e-01,\n",
       "          1.18500970e+00],\n",
       "       [ -1.73673948e-01,  -1.28197243e+00,   7.05892939e-01,\n",
       "          1.05353673e+00],\n",
       "       [ -5.25060772e-02,  -5.87763531e-01,   7.62758643e-01,\n",
       "          1.57942861e+00],\n",
       "       [  6.74501145e-01,   3.37848329e-01,   8.76490051e-01,\n",
       "          1.44795564e+00],\n",
       "       [  7.95669016e-01,  -1.24957601e-01,   9.90221459e-01,\n",
       "          7.90590793e-01],\n",
       "       [  2.24968346e+00,   1.72626612e+00,   1.67260991e+00,\n",
       "          1.31648267e+00],\n",
       "       [  2.24968346e+00,  -1.05056946e+00,   1.78634131e+00,\n",
       "          1.44795564e+00],\n",
       "       [  1.89829664e-01,  -1.97618132e+00,   7.05892939e-01,\n",
       "          3.96171883e-01],\n",
       "       [  1.28034050e+00,   3.37848329e-01,   1.10395287e+00,\n",
       "          1.44795564e+00],\n",
       "       [ -2.94841818e-01,  -5.87763531e-01,   6.49027235e-01,\n",
       "          1.05353673e+00],\n",
       "       [  2.24968346e+00,  -5.87763531e-01,   1.67260991e+00,\n",
       "          1.05353673e+00],\n",
       "       [  5.53333275e-01,  -8.19166497e-01,   6.49027235e-01,\n",
       "          7.90590793e-01],\n",
       "       [  1.03800476e+00,   5.69251294e-01,   1.10395287e+00,\n",
       "          1.18500970e+00],\n",
       "       [  1.64384411e+00,   3.37848329e-01,   1.27454998e+00,\n",
       "          7.90590793e-01],\n",
       "       [  4.32165405e-01,  -5.87763531e-01,   5.92161531e-01,\n",
       "          7.90590793e-01],\n",
       "       [  3.10997534e-01,  -1.24957601e-01,   6.49027235e-01,\n",
       "          7.90590793e-01],\n",
       "       [  6.74501145e-01,  -5.87763531e-01,   1.04708716e+00,\n",
       "          1.18500970e+00],\n",
       "       [  1.64384411e+00,  -1.24957601e-01,   1.16081857e+00,\n",
       "          5.27644853e-01],\n",
       "       [  1.88617985e+00,  -5.87763531e-01,   1.33141568e+00,\n",
       "          9.22063763e-01],\n",
       "       [  2.49201920e+00,   1.72626612e+00,   1.50201279e+00,\n",
       "          1.05353673e+00],\n",
       "       [  6.74501145e-01,  -5.87763531e-01,   1.04708716e+00,\n",
       "          1.31648267e+00],\n",
       "       [  5.53333275e-01,  -5.87763531e-01,   7.62758643e-01,\n",
       "          3.96171883e-01],\n",
       "       [  3.10997534e-01,  -1.05056946e+00,   1.04708716e+00,\n",
       "          2.64698913e-01],\n",
       "       [  2.24968346e+00,  -1.24957601e-01,   1.33141568e+00,\n",
       "          1.44795564e+00],\n",
       "       [  5.53333275e-01,   8.00654259e-01,   1.04708716e+00,\n",
       "          1.57942861e+00],\n",
       "       [  6.74501145e-01,   1.06445364e-01,   9.90221459e-01,\n",
       "          7.90590793e-01],\n",
       "       [  1.89829664e-01,  -1.24957601e-01,   5.92161531e-01,\n",
       "          7.90590793e-01],\n",
       "       [  1.28034050e+00,   1.06445364e-01,   9.33355755e-01,\n",
       "          1.18500970e+00],\n",
       "       [  1.03800476e+00,   1.06445364e-01,   1.04708716e+00,\n",
       "          1.57942861e+00],\n",
       "       [  1.28034050e+00,   1.06445364e-01,   7.62758643e-01,\n",
       "          1.44795564e+00],\n",
       "       [ -5.25060772e-02,  -8.19166497e-01,   7.62758643e-01,\n",
       "          9.22063763e-01],\n",
       "       [  1.15917263e+00,   3.37848329e-01,   1.21768427e+00,\n",
       "          1.44795564e+00],\n",
       "       [  1.03800476e+00,   5.69251294e-01,   1.10395287e+00,\n",
       "          1.71090158e+00],\n",
       "       [  1.03800476e+00,  -1.24957601e-01,   8.19624347e-01,\n",
       "          1.44795564e+00],\n",
       "       [  5.53333275e-01,  -1.28197243e+00,   7.05892939e-01,\n",
       "          9.22063763e-01],\n",
       "       [  7.95669016e-01,  -1.24957601e-01,   8.19624347e-01,\n",
       "          1.05353673e+00],\n",
       "       [  4.32165405e-01,   8.00654259e-01,   9.33355755e-01,\n",
       "          1.44795564e+00],\n",
       "       [  6.86617933e-02,  -1.24957601e-01,   7.62758643e-01,\n",
       "          7.90590793e-01]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit_transform(data) #check out preprocessing module of sklearn to learn more about preprocessing in ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now let us divide data into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=42, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train a model\n",
    "model = SVC()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.support_vectors_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.8,  1.9,  0.4],\n",
       "       [ 4.8,  3.4,  1.9,  0.2],\n",
       "       [ 5.5,  4.2,  1.4,  0.2],\n",
       "       [ 4.5,  2.3,  1.3,  0.3],\n",
       "       [ 5.8,  4. ,  1.2,  0.2],\n",
       "       [ 5.6,  3. ,  4.5,  1.5],\n",
       "       [ 5. ,  2. ,  3.5,  1. ],\n",
       "       [ 5.4,  3. ,  4.5,  1.5],\n",
       "       [ 6.7,  3. ,  5. ,  1.7],\n",
       "       [ 5.9,  3.2,  4.8,  1.8],\n",
       "       [ 5.1,  2.5,  3. ,  1.1],\n",
       "       [ 6. ,  2.7,  5.1,  1.6],\n",
       "       [ 6.3,  2.5,  4.9,  1.5],\n",
       "       [ 6.1,  2.9,  4.7,  1.4],\n",
       "       [ 6.5,  2.8,  4.6,  1.5],\n",
       "       [ 7. ,  3.2,  4.7,  1.4],\n",
       "       [ 6.1,  3. ,  4.6,  1.4],\n",
       "       [ 5.5,  2.6,  4.4,  1.2],\n",
       "       [ 4.9,  2.4,  3.3,  1. ],\n",
       "       [ 6.9,  3.1,  4.9,  1.5],\n",
       "       [ 6.3,  2.3,  4.4,  1.3],\n",
       "       [ 6.3,  2.8,  5.1,  1.5],\n",
       "       [ 7.7,  2.8,  6.7,  2. ],\n",
       "       [ 6.3,  2.7,  4.9,  1.8],\n",
       "       [ 7.7,  3.8,  6.7,  2.2],\n",
       "       [ 5.7,  2.5,  5. ,  2. ],\n",
       "       [ 6. ,  3. ,  4.8,  1.8],\n",
       "       [ 5.8,  2.7,  5.1,  1.9],\n",
       "       [ 6.2,  3.4,  5.4,  2.3],\n",
       "       [ 6.1,  2.6,  5.6,  1.4],\n",
       "       [ 6. ,  2.2,  5. ,  1.5],\n",
       "       [ 6.3,  3.3,  6. ,  2.5],\n",
       "       [ 6.2,  2.8,  4.8,  1.8],\n",
       "       [ 6.9,  3.1,  5.4,  2.1],\n",
       "       [ 6.5,  3. ,  5.2,  2. ],\n",
       "       [ 7.2,  3. ,  5.8,  1.6],\n",
       "       [ 5.6,  2.8,  4.9,  2. ],\n",
       "       [ 5.9,  3. ,  5.1,  1.8],\n",
       "       [ 4.9,  2.5,  4.5,  1.7]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:verdana; font-size:15px\">Till now, we see problems where input data can be seperated by linear hyperplane. But what is data points are not linearly seperable as shown below?</p>\n",
    "![title](../images/svm5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:verdana; font-size:15px\">To solve this type of problems where data can not be seperated linearly, we add new feature. For example, let us add new feature z = x<sup>2</sup> + y<sup>2</sup>. Now, if we plot data points on x and z axis we get :</p>\n",
    "![title](../images/svm3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:verdana; font-size:15px\"> As you can see, now we can have a linear hyperplane that can seperate data points very well. Do we need to add this additional feature manually? And the answer is no. We use the technique called <b> Kernel Trick</b>. Kernel trick is nothing but a set of functions which takes low-dimensional input space and transform it into high-dimensional space where data points are linearly seperable. These functions are called kernels. Widely used kernels are Radial Basis Function Kernel, Polynomial Kernel, Sigmoid kernel, etc.<br><br>\n",
    "\n",
    "Let us implement this in sklearn.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we have already imported libs and dataset\n",
    "model2= SVC(kernel=\"rbf\", gamma=0.2)\n",
    "model2.fit(X_train, y_train)\n",
    "model2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p style=\"font-family:verdana; font-size:15px\"> We can have different decision boundary for different kernels and gamma values. Here is the screenshot from scikit-learn website.</p>\n",
    "![title](../images/svm6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:verdana; font-size:15px\" id=\"dt\">Decision Tree is the supervised learning algorithm which can be used for classification as well as regression problems. Decision Tree is very popular learning algorithm because of its interpretability. In this method, we split population into set of homogeneous sets by asking set of questions. Consider a problem where we want to decide what to do on a particular day. We can design a decision tree as follows : (Source: Python Machine Learning by Sebastian Raschka)</p>\n",
    "![title](../images/dt1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:verdana; font-size:15px\">There are two types of nodes in above figure. One with gray background and one with white background. Nodes with gray background color are terminal nodes. Each edge represents a decision, for i.e, weather is sunny or not?. In above figure, we have four possible classes and each terminal node represents one of them. In above example, features are - work(yes/no), weather(i.e sunny), friends busy?(yes/no). Once we have a decision tree(we haven't seen how to construct them yet) we can predict the outcome. For example, if our input is work(no),weather(rainy),freinds busy?(no) then we will opt for <b> Go to movies</b>.<br><br>\n",
    "\n",
    "But, how do we construct decision tree? We start with root node and split dataset on a feature that results in largest <b>information gain</b>. We repeat this process of splitting on child node unitl we get nodes which are pure means they contain samples of a once class. We need a way to compute the impurity at each node. Before going into that let us first define information gain. Here, our objective is to maximize the information gain at each split. We define information gain as follows : \n",
    "<br><br>\n",
    "\\begin{align}\n",
    "IG(D_p,f) = I(D_p) - \\sum_{j=1}^{m} \\frac{N_j}{N_p} I(D_j)\n",
    "\\end{align}\n",
    "\n",
    "Here, f is the feature to perform the split, D<sub>p</sub> and D<sub>j</sub> are the dataset of the parent and jth child node, I is the impurity measure (i.e entropy, gini index), N<sub>p</sub> is the total number of smples at the parent node and N<sub>j</sub> is the number of samples in the jth child node. Information gain is the difference between the impurity of parent node and the sum of the child node impurities. However, for simplicity and to reduce the combinatorial search space, most machine learning libraries implement binary decision tree, that is, each parent node has two child nodes. Say two child nodes are, D<sub>left</sub> and D<sub>right</sub> then information gain will be:\n",
    "<br><br>\n",
    "\\begin{align}\n",
    "IG(D_p, f) = I(D_p) - \\frac{N_{left}}{N_p} I(D_{left}) - \\frac{N_{right}}{N_p} I(D_{right})\n",
    "\\end{align}\n",
    "<Br><br>\n",
    "Commonly used impurity measures or splitting criteria are <b> Gini Impurity </b>, <b> Entropy </b> and <b> Classification Error</b>. Here, we'll only discuss about entropy.\n",
    "\n",
    "For all non-empty classes (p(i | t) not equals to zero), entropy can be defined as <br>\n",
    "<br><br>\n",
    "\\begin{align}\n",
    "I_H(t) = -\\sum_{i=1}^{C}  p(i | t)  log_2 p(i | t)\n",
    "\\end{align}\n",
    "<br><br>\n",
    "Where p(i | t) is the part of the samples that belong to the class i for a particular node t and C is the number of classes. The entropy is 0 if all samples at a node belong to the same class and the entropy is maximum if the classes are distributed uniformly. For example, if we have 100 samples and 2 classes then entropy will be maximum if 50 samples belong to one class and remaining belong to second class.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#entropy in numpy\n",
    "import numpy as np\n",
    "def entropy(p):\n",
    "    return -p * np.log2(p) - (1-p) * np.log2((1-p)) #for binary class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../images/entropy.png)\n",
    "<p style=\"font-family:verdana; font-size:15px\">\n",
    "As you can see, entropy is maximum when p(i=1 | t ) and p(i=0 | t)=0.5. And entropy is minimum when all the samples belong to the same class. We define Gini Impurity as :\n",
    "<br><BR>\n",
    "\\begin{align}\n",
    "I_g(t) = \\sum_{i=1}^{C} p(i | t) (1 - p(i | t))\n",
    "\\end{align}<br><Br>\n",
    "Now let us implement Decision Tree in sklearn.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=3,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=42, splitter='best')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import libs and dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = load_iris()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=42)\n",
    "\n",
    "model = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3, random_state=42)\n",
    "#decision trees are prone to overfitting thats why we remove some sub-nodes of the tree, that's called \"pruning\"\n",
    "#here, we control depth of the tree using max_depth attribute\n",
    "#other option for criterion is \"gini\"\n",
    "#random_state- just to make sure we get same results each time we run this code\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97777777777777775"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the model\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:verdana; font-size:15px\">\n",
    "We can also save the decision tree which our model has built. You need to have <a href=\"http://www.graphviz.org/Download.php\">Graph Viz </a> and <a href=\"https://pypi.python.org/pypi/pydotplus\">pydotplus</a> installed in your system.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals.six import StringIO  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "dot_data = StringIO() \n",
    "export_graphviz(model, out_file=dot_data) \n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) \n",
    "graph.write_pdf(\"iris.pdf\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnQAAAH8CAYAAABYYjLnAAAAAXNSR0IArs4c6QAAAARnQU1BAACx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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"../images/tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:verdana; font-size:15px\">\n",
    "Some of the advantages of Decision Trees are they are easy to understand because of their interpretability, less data cleaning is required, it can handle both numerical and categorical data. One of the problem with decision tree is overfitting which can be solved by pruning the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:verdana; font-size:15px\" id=\"ensemble\">\n",
    "Till now, we discussed methods in which there was only one hypothesis. In other words, we were training only one model for one problem. In ensemble learning, we train a set of models to achieve better accuracy and stability. At prediction time we combine the prediction of all. In case of classification, we take vote of each model and in case of regression, we take the mean of the prediction of each model. We run multiple models on the data and use the aggregate prediction, which is better than a single model.<br><br></p>\n",
    "<p style=\"font-family:verdana; font-size:15px\">\n",
    "<b> Bagging (Bootstrap aggregating)</b> is a technique used to reduce the variance of a prediction by combining the result of multiple classifiers. In baggin, first we create multiple datasets from the original dataset. Please note, these datasets have random number of samples and random number of features. Then we build classifier on each dataset. For prediction, we use combine predictions of models.<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Random Forest</b> is the most common type of Ensemble Learning. Basically, it is a collection of decision trees. We repeatedly select data from the data set with replacement (which is also known as <b> bootstrapping</b>)and build a Decision Tree with each new sample. One thing to note here is that each node of the Decision Tree is limited to only considering splits on random subsets of the features. If features were not chosen randomly, Decision Trees in our forest could become highly correlated. There are plethora of advantages of random forest such as they are fast to train, requires no input preparation. One of the disadvantage of random forest is that our model may become too large.<br><br>\n",
    "\n",
    "</p>\n",
    "<p style=\"font-family:verdana; font-size:15px\">\n",
    "We hold out one third of the dataset before generating multiple datasets to use in testing. These samples are called <b> out of bag samples</b> and error estimated with these samples is known as <b> out of bag error</b>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 0.0333333333333\n",
      "31 0.0333333333333\n",
      "32 0.0333333333333\n",
      "33 0.0333333333333\n",
      "34 0.0333333333333\n",
      "35 0.0333333333333\n",
      "36 0.0333333333333\n",
      "37 0.04\n",
      "38 0.04\n",
      "39 0.04\n",
      "40 0.04\n",
      "41 0.0333333333333\n",
      "42 0.0333333333333\n",
      "43 0.0333333333333\n",
      "44 0.04\n",
      "45 0.0466666666667\n",
      "46 0.04\n",
      "47 0.0333333333333\n",
      "48 0.04\n",
      "49 0.04\n",
      "50 0.04\n",
      "51 0.04\n",
      "52 0.0333333333333\n",
      "53 0.0333333333333\n",
      "54 0.0466666666667\n",
      "55 0.0466666666667\n",
      "56 0.04\n",
      "57 0.04\n",
      "58 0.04\n",
      "59 0.04\n",
      "60 0.04\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "min_estimator = 30  #min number of trees to be built\n",
    "max_estimator = 60  #max number of trees to be built\n",
    "rf = RandomForestClassifier(criterion=\"entropy\", warm_start=True, oob_score=True,random_state=42)\n",
    "for i in range(min_estimator, max_estimator+1):\n",
    "    rf.set_params(n_estimators=i)\n",
    "    rf.fit(X,y)   #do not need to seperate training and testing set\n",
    "    oob_score = 1 - rf.oob_score_\n",
    "    print(i, oob_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rf.score(X,rf.predict(X)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p style=\"font-family:verdana; font-size:15px\">\n",
    "<b>Boosting </b> is a technique used to reduce bias as well as variance in supervised learning. Bias means how much on an average are the predicted values different from the actual value and variance means how different will the predictions of the model be at the same point if different samples are taken from the same population. Boosting refers to a set of machine learning algorithms which convert <b>weak learners</b> into <b>strong learner</b>. This was inspired by a question- Can a set of weak learners create a single strong learner? Gradient Boosting, AdaBoost and XGBoost are examples of boosting algorithms. Boosting algorithms such as xgboost are the secret to win machine learning competitions. Here, I'm not going deeper into boosting algorithms. But they are important. See <a hef=\"http://mccormickml.com/2013/12/13/adaboost-tutorial/\">here</a>, <a href=\"http://math.mit.edu/~rothvoss/18.304.3PM/Presentations/1-Eric-Boosting304FinalRpdf.pdf\">here</a> for AdaBoost and see <a href=\"http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf\">here</a>, <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/\">here</a> for Gradient Boosting. See <a href=\"http://xgboost.readthedocs.io/en/latest/model.html\">here</a> for XGBoost</p>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#load the dataset\n",
    "dataset = load_iris()\n",
    "\n",
    "#split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, random_state=42, test_size=0.33)\n",
    "#optioanl - Standardize inputs using StandardScaler\n",
    "#instantiate a model\n",
    "model = AdaBoostClassifier(n_estimators=150, random_state=42)\n",
    "#train a model\n",
    "model.fit(X_train, y_train)  \n",
    "\n",
    "print(\"accuracy: \", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error:  8.85292814297\n"
     ]
    }
   ],
   "source": [
    "#we use most of the algorithms for classification algorithm, for this lets go for regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#load the dataset\n",
    "dataset = load_boston()\n",
    "\n",
    "#split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, random_state=42, test_size=0.33)\n",
    "#optioanl - Standardize inputs using StandardScaler\n",
    "#instantiate a model\n",
    "model = GradientBoostingRegressor(n_estimators=500,learning_rate=0.01, random_state=42)\n",
    "#train a model\n",
    "model.fit(X_train, y_train)  \n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"Mean squared error: \", mean_squared_error(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:verdana; font-size:15px\" id=\"ex\">\n",
    "In <a href=\"Sentiment%20Analysis.ipynb\">this</a> exercise, we will implement a sentiment analysis model which can detect the sentiment from a text. We will also go through some feature extraction techniques and learn how to use textual data in machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
